{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mask_RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkyIntelligence/Springboard/blob/master/Mask_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czTAuNvHJq8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CS5_4WS18fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tf.keras.layers import *\n",
        "import numpy as np\n",
        "\n",
        "################################################################################\n",
        "# Utility functions ... TODO: separate out into a different file\n",
        "################################################################################\n",
        "\n",
        "def trim_zeros(boxes, name='trim_zeros'):\n",
        "  \"\"\"\n",
        "  Often boxes are represented with tensors of shape [batch, N, 4] and\n",
        "  are padded with zeros. This removes zero boxes.\n",
        "  input: boxes: [batch, (instances), 4] potentially ragged tensor of boxes.\n",
        "\n",
        "  returns:\n",
        "  boxes: [batch, (non-zero instances), 4]\n",
        "  non_zeros: [batch, (instances)] a 2D boolean mask identifying the boxes in each image to keep\n",
        "  \"\"\"\n",
        "  non_zeros = tf.cast(tf.reduce_sum(tf.abs(boxes), axis=2), tf.bool)\n",
        "  boxes = tf.ragged.boolean_mask(boxes, non_zeros)\n",
        "  return boxes, non_zeros\n",
        "\n",
        "def overlaps(boxes1, boxes2):\n",
        "  \"\"\"\n",
        "  Computes IoU overlaps between two sets of boxes.\n",
        "  boxes1, boxes2: [batch, (instances), (y1, x1, y2, x2)].\n",
        "  \"\"\"\n",
        "  # Need to compute every IOU combination per batch (both ragged tensors)\n",
        "  # i.e. repeat boxes1 instances boxes2 instances times, and vice versa\n",
        "\n",
        "\n",
        "# Config Globals for now...\n",
        "TOP_DOWN_PYRAMID_SIZE = 256\n",
        "BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
        "RPN_ANCHOR_SCALES = (32, 64, 128, 256, 512)\n",
        "RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
        "RPN_ANCHOR_STRIDE = 1\n",
        "BATCH_SIZE = 2 #Hardcoded for now should be at most IMAGES_PER_GPU * GPU's (How does this change for TPUs?)\n",
        "POST_NMS_ROIS_TRAINING = 2000\n",
        "RPN_NMS_THRESHOLD = 0.7\n",
        "RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
        "# ROIs kept after tf.nn.top_k and before non-maximum suppression\n",
        "PRE_NMS_LIMIT = 6000\n",
        "MAX_GT_INSTANCES = 100\n",
        "\n",
        "################################################################################\n",
        "# Resnet Class\n",
        "################################################################################\n",
        "\n",
        "# Resnet Classes to produce the Feature Pyramid Network\n",
        "# adapted from https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py\n",
        "\n",
        "\n",
        "class IdentityBlock(Layer):\n",
        "  def __init__(self, kernel_size, filter_sizes, stage, block, use_bias=True):\n",
        "    \"\"\"The identity_block is the block that has no conv layer at shortcut\n",
        "    # Parameters:\n",
        "      kernel_size: default 3, the kernel size of middle conv layer at main path\n",
        "      filter_sizes: list of integers, filter sizes of 3 conv layers at main path\n",
        "      stage: integer, current stage label, used for generating layer names\n",
        "      block: 'a','b'..., current block label, used for generating layer names\n",
        "      use_bias: Boolean. To use or not use a bias in conv layers\n",
        "    \"\"\"\n",
        "    super(IdentityBlock, self).__init__()\n",
        "\n",
        "    self._kernel_size = kernel_size\n",
        "    self._filter_size_1, self._filter_size_2, self._filter_size_3 = filter_sizes\n",
        "    self._stage = stage\n",
        "    self._block = block\n",
        "    self._conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    self._bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    self._conv_layer_1 = Conv2D(self._filter_size_1, (1, 1),\n",
        "                      name = self._conv_name_base + '2a', use_bias=use_bias)\n",
        "    self._conv_layer_2 = Conv2D(self._filter_size_2, (kernel_size, kernel_size), padding='same',\n",
        "                      name = self._conv_name_base + '2b', use_bias=use_bias)\n",
        "    self._conv_layer_3 = Conv2D(self._filter_size_3, (1, 1),\n",
        "                      name = self._conv_name_base + '2c', use_bias=use_bias)\n",
        "    \n",
        "    self._batch_norm_1 = BatchNormalization(name=self._bn_name_base + '2a')\n",
        "    self._batch_norm_2 = BatchNormalization(name=self._bn_name_base + '2b')\n",
        "    self._batch_norm_3 = BatchNormalization(name=self._bn_name_base + '2c')\n",
        "\n",
        "    self._act_layer_1 = Activation('relu')\n",
        "    self._act_layer_2 = Activation('relu')\n",
        "    self._act_layer_out = Activation('relu', name='res' + str(stage) + block + '_out')\n",
        "\n",
        "  def call(self, x, train_bn=True):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      x: the input tensor\n",
        "      train_bn: boolean, whether to train the batch normalization layer or not\n",
        "    \"\"\"\n",
        "    main = self._conv_layer_1(x)\n",
        "    main = self._batch_norm_1(main, training=train_bn)\n",
        "    main = self._act_layer_1(main)\n",
        "    main = self._conv_layer_2(main)\n",
        "    main = self._batch_norm_2(main, training=train_bn)\n",
        "    main = self._act_layer_2(main)\n",
        "    main = self._conv_layer_3(main)\n",
        "    main = self._batch_norm_3(main, training=train_bn)\n",
        "    main = main + x\n",
        "    main = self._act_layer_out(main)\n",
        "\n",
        "    return main\n",
        "\n",
        "\n",
        "class ConvBlock(Layer):\n",
        "  def __init__(self, kernel_size, filter_sizes, stage, block, strides=(2, 2),\n",
        "               use_bias=True):\n",
        "    \"\"\"conv_block is the block that has a conv layer at shortcut\n",
        "    # Parameters\n",
        "      kernel_size: default 3, the kernel size of middle conv layer at main path\n",
        "      filters: list of integers, the nb_filters of 3 conv layer at main path\n",
        "      stage: integer, current stage label, used for generating layer names\n",
        "      block: 'a','b'..., current block label, used for generating layer names\n",
        "      use_bias: Boolean. To use or not use a bias in conv layers.\n",
        "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
        "    And the shortcut should have subsample=(2,2) as well\n",
        "    \"\"\"\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self._kernel_size = kernel_size\n",
        "    self._filter_size1, self._filter_size2, self._filter_size3 = filter_sizes\n",
        "    self._stage = stage\n",
        "    self._block = block\n",
        "    self._conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    self._bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    self._conv_layer_1 = Conv2D(self._filter_size1, (1, 1), strides=strides,\n",
        "                      name = self._conv_name_base + '2a', use_bias=use_bias)\n",
        "    self._conv_layer_2 = Conv2D(self._filter_size2, (kernel_size, kernel_size), padding='same',\n",
        "                      name = self._conv_name_base + '2b', use_bias=use_bias)\n",
        "    self._conv_layer_3 = Conv2D(self._filter_size3, (1, 1),\n",
        "                      name = self._conv_name_base + '2c', use_bias=use_bias)\n",
        "    self._conv_layer_sc = Conv2D(self._filter_size3, (1, 1), strides=strides,\n",
        "                      name = self._conv_name_base + '1', use_bias=use_bias)\n",
        "    \n",
        "    self._batch_norm_1 = BatchNormalization(name=self._bn_name_base + '2a')\n",
        "    self._batch_norm_2 = BatchNormalization(name=self._bn_name_base + '2b')\n",
        "    self._batch_norm_3 = BatchNormalization(name=self._bn_name_base + '2c')\n",
        "    self._batch_norm_sc = BatchNormalization(name=self._bn_name_base + '1')\n",
        "\n",
        "    self._act_layer_1 = Activation('relu')\n",
        "    self._act_layer_2 = Activation('relu')\n",
        "    self._act_layer_out = Activation('relu', name='res' + str(stage) + block + '_out')\n",
        "\n",
        "  def call(self, x, train_bn=True):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      x: the input tensor\n",
        "      train_bn: boolean, whether to train the batch normalization layer or not\n",
        "    \"\"\"\n",
        "    main = self._conv_layer_1(x)\n",
        "    main = self._batch_norm_1(main, training=train_bn)\n",
        "    main = self._act_layer_1(main)\n",
        "    main = self._conv_layer_2(main)\n",
        "    main = self._batch_norm_2(main, training=train_bn)\n",
        "    main = self._act_layer_2(main)\n",
        "    main = self._conv_layer_3(main)\n",
        "    main = self._batch_norm_3(main, training=train_bn)\n",
        "\n",
        "    shortcut = self._conv_layer_sc(x)\n",
        "    shortcut = self._batch_norm_sc(shortcut)\n",
        "\n",
        "    main = main + shortcut\n",
        "    main = self._act_layer_out(main)\n",
        "\n",
        "    return main\n",
        "\n",
        "\n",
        "class ResNetLayer(Layer):\n",
        "  def __init__(self, architecture, stage_5=False):\n",
        "    \"\"\"Builds a resnet\n",
        "    # Parameters:\n",
        "      architecture: Can be resnet50 or resnet101\n",
        "      stage_5: Boolean. If False, stage 5 of the network is not included\n",
        "    \"\"\"\n",
        "    assert architecture in ['resnet50', 'resnet101']\n",
        "    super(ResNet, self).__init__()\n",
        "\n",
        "    self._stage_5 = stage_5\n",
        "    \n",
        "    self._zero_padding_layer = ZeroPadding2D((3, 3))\n",
        "    self._conv_layer = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=True)   \n",
        "    self._batch_norm = BatchNormalization(name='bn_conv1')   \n",
        "    self._act_layer = Activation('relu')    \n",
        "    self._max_pool_layer = MaxPool2D((3, 3), strides=(2, 2), padding=\"same\")\n",
        "    \n",
        "    self._conv_block_2a = ConvBlock(3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    self._id_block_2b = IdentityBlock(3, [64, 64, 256], stage=2, block='b')\n",
        "    self._id_block_2c = IdentityBlock(3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    self._conv_block_3a = ConvBlock(3, [128, 128, 512], stage=3, block='a')\n",
        "    self._id_block_3b = IdentityBlock(3, [128, 128, 512], stage=3, block='b')\n",
        "    self._id_block_3c = IdentityBlock(3, [128, 128, 512], stage=3, block='c')\n",
        "    self._id_block_3d = IdentityBlock(3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    self._conv_block_4a = ConvBlock(3, [256, 256, 1024], stage=4, block='a')\n",
        "    self._s4_block_count = {'resnet50': 5, \"resnet101\": 22}[architecture]\n",
        "    self._s4_id_blocks = []\n",
        "    for i in range(self._s4_block_count):\n",
        "      self._s4_id_blocks.append(IdentityBlock(3, [256, 256, 1024], stage=4, block=chr(98 + i)))\n",
        "\n",
        "    if stage_5:\n",
        "      self._conv_block_5a = ConvBlock(3, [512, 512, 2048], stage=5, block='a')\n",
        "      self._id_block_5b = IdentityBlock(3, [512, 512, 2048], stage=5, block='b')\n",
        "      self._id_block_5c = IdentityBlock(3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "  def call(self, x, train_bn=True):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      x: the input tensor\n",
        "      train_bn: boolean, whether to train the batch normalization layer or not\n",
        "    \"\"\"\n",
        "    # Stage 1\n",
        "    x = self._zero_padding_layer(x)\n",
        "    x = self._conv_layer(x)\n",
        "    x = self._batch_norm(x, training=train_bn)\n",
        "    x = self._act_layer(x)\n",
        "    C1 = x = self._max_pool_layer(x)\n",
        "    # Stage 2\n",
        "    x = self._conv_block_2a(x, train_bn=train_bn)\n",
        "    x = self._id_block_2b(x, train_bn=train_bn)\n",
        "    C2 = x = self._id_block_2c(x, train_bn=train_bn)\n",
        "    # Stage 3\n",
        "    x = self._conv_block_3a(x, train_bn=train_bn)\n",
        "    x = self._id_block_3b(x, train_bn=train_bn)\n",
        "    x = self._id_block_3c(x, train_bn=train_bn)\n",
        "    C3 = x = self._id_block_3d(x, train_bn=train_bn)\n",
        "    # Stage 4\n",
        "    x = self._conv_block_4a(x, train_bn=train_bn)\n",
        "    for i in range(self._s4_block_count):\n",
        "      x = self._s4_id_blocks[i](x, train_bn=train_bn))\n",
        "    C4 = x\n",
        "    # Stage 5\n",
        "    if self._stage_5:\n",
        "      x = self._conv_block_5a(x, train_bn=train_bn)\n",
        "      x = self._id_block_5b(x, train_bn=train_bn)\n",
        "      C5 = x = self._id_block_5c(x, train_bn=train_bn)\n",
        "    else:\n",
        "      C5 = None\n",
        "    \n",
        "    return [C1, C2, C3, C4, C5]\n",
        "\n",
        "\n",
        "class RegionProposalNetwork(Layer):\n",
        "  def __init__(self, anchor_stride, anchors_per_location):\n",
        "\n",
        "    self._conv_shared = Conv2D(512, (3, 3), padding='same', \n",
        "      activation='relu', strides=anchor_stride, name='rpn_conv_shared')\n",
        "    \n",
        "    self._conv_class = Conv2D(anchors_per_location * 2, (1, 1), padding='valid',\n",
        "                              activation='linear', name='rpn_class_raw')\n",
        "    \n",
        "    self._class_softmax = Activation('softmax', name='rpn_class_xxx')\n",
        "\n",
        "    self._conv_bbox = Conv2D(anchors_per_location * 4, (1, 1), padding='valid',\n",
        "                             activation='linear', name='rpn_bbox_pred')\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      x: feature_map\n",
        "    \"\"\"\n",
        "    x = self._conv_shared(x)\n",
        "    # Anchor Score BG/FG. [batch, height, width, anchors per location * 2]\n",
        "    class_logits = self._conv_class(x)\n",
        "    # Reshape to [batch, anchors, 2]\n",
        "    class_logits.set_shape(class_logits.get_shape()[0], -1, 2)\n",
        "    # Softmax on last dimension of BG/FG\n",
        "    class_probs = self._class_softmax(class_logits)\n",
        "\n",
        "    # Bounding box refinement. [batch, H, W, anchors per location * depth]\n",
        "    # where depth is [x, y, log(w), log(h)]\n",
        "    bbox = self._conv_bbox(x)\n",
        "\n",
        "    # Reshape to [batch, anchors, 4]\n",
        "    bbox.set_shape(bbox.get_shape()[0], -1, 4)\n",
        "\n",
        "    return [class_logits, class_probs, bbox]\n",
        "\n",
        "\n",
        "class MaskRCNN(tf.keras.Model):\n",
        "  \"\"\"\n",
        "  Model Class to implement MaskRCNN\n",
        "  \"\"\"\n",
        "  def __init__(self, mode='training', rn_arch='resnet50', rn_stage_5=False):\n",
        "    super(MaskRCNN, self).__init__()\n",
        "\n",
        "    self.mode = mode\n",
        "\n",
        "    self._anchor_cache = {}\n",
        "\n",
        "    self._resnet_layer = ResNetLayer(rn_arch, rn_stage_5)\n",
        "\n",
        "    self._conv_layer_fpn_c5p5 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c5p5')\n",
        "    self._conv_layer_fpn_c4p4 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c4p4')\n",
        "    self._conv_layer_fpn_c3p3 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c3p3')\n",
        "    self._conv_layer_fpn_c2p2 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (1, 1), name='fpn_c2p2')\n",
        "\n",
        "    self._conv_layer_fpn_p2 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (3, 3), name='fpn_p2')\n",
        "    self._conv_layer_fpn_p3 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (3, 3), name='fpn_p3')\n",
        "    self._conv_layer_fpn_p4 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (3, 3), name='fpn_p4')\n",
        "    self._conv_layer_fpn_p5 = Conv2D(TOP_DOWN_PYRAMID_SIZE, (3, 3), name='fpn_p5')\n",
        "\n",
        "    self._upsample_layer_fpn_p5 = UpSampling2D(size=(2, 2), name='fpn_p5upsampled')\n",
        "    self._upsample_layer_fpn_p4 = UpSampling2D(size=(2, 2), name='fpn_p4upsampled')\n",
        "    self._upsample_layer_fpn_p3 = UpSampling2D(size=(2, 2), name='fpn_p3upsampled')\n",
        "\n",
        "    self._maxpool_layer = MaxPooling2D(pool_size=(1, 1), strides=2, name='fpn_p6')\n",
        "\n",
        "    self._RPN = RegionProposalNetwork(ANCHOR_STRIDE, ANCHOR_PER_LOCATION)\n",
        "\n",
        "\n",
        "  def _compute_backbone_shapes(self, image_shape):\n",
        "    return np.array([[int(math.ceil(image_shape[0] / stride)),\n",
        "                      int(math.ceil(image_shape[1] / stride))]\n",
        "                     for stride in BACKBONE_STRIDES])\n",
        "\n",
        "\n",
        "  def _generate_anchors(self, scales, ratios, shape, feature_stride,\n",
        "                        anchor_stride):\n",
        "    \"\"\"\n",
        "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
        "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
        "    shape: [height, width] spatial shape of the feature map over which\n",
        "            to generate anchors.\n",
        "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
        "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
        "        value is 2 then generate anchors for every other feature map pixel.\n",
        "    \"\"\"\n",
        "    # Get all combinations of scales and ratios\n",
        "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
        "    scales = scales.flatten()\n",
        "    ratios = ratios.flatten()\n",
        "\n",
        "    # Enumerate heights and widths from scales and ratios\n",
        "    heights = scales / np.sqrt(ratios)\n",
        "    widths = scales * np.sqrt(ratios)\n",
        "\n",
        "    # Enumerate shifts in feature space\n",
        "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
        "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
        "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
        "\n",
        "    # Enumerate combinations of shifts, widths, and heights\n",
        "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
        "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
        "\n",
        "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
        "    box_centers = np.stack(\n",
        "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
        "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
        "\n",
        "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
        "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
        "                            box_centers + 0.5 * box_sizes], axis=1)\n",
        "    return boxes\n",
        "\n",
        "\n",
        "  def _generate_pyramid_anchors(self, scales, ratios, feature_shapes,\n",
        "                                feature_strides, anchor_stride):\n",
        "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
        "    is associated with a level of the pyramid, but each ratio is used in\n",
        "    all levels of the pyramid.\n",
        "    Returns:\n",
        "    anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted\n",
        "        with the same order of the given scales. So, anchors of scale[0] come\n",
        "        first, then anchors of scale[1], and so on.\n",
        "    \"\"\"\n",
        "    # Anchors\n",
        "    # [anchor_count, (y1, x1, y2, x2)]\n",
        "    anchors = []\n",
        "    for i in range(len(scales)):\n",
        "        anchors.append(self._generate_anchors(scales[i], ratios, feature_shapes[i],\n",
        "                                        feature_strides[i], anchor_stride))\n",
        "    return np.concatenate(anchors, axis=0)\n",
        "\n",
        "\n",
        "  def _norm_boxes(self, boxes, shape):\n",
        "    \"\"\"Converts boxes from pixel coordinates to normalized coordinates.\n",
        "    boxes: [N, (y1, x1, y2, x2)] in pixel coordinates\n",
        "    shape: [..., (height, width)] in pixels\n",
        "    Note: In pixel coordinates (y2, x2) is outside the box. But in normalized\n",
        "    coordinates it's inside the box.\n",
        "    Returns:\n",
        "        [N, (y1, x1, y2, x2)] in normalized coordinates\n",
        "    \"\"\"\n",
        "    h, w = shape\n",
        "    scale = np.array([h - 1, w - 1, h - 1, w - 1])\n",
        "    shift = np.array([0, 0, 1, 1])\n",
        "    return np.divide((boxes - shift), scale).astype(np.float32)\n",
        "\n",
        "\n",
        "  def _get_anchors(self, image_shape):\n",
        "    \"\"\"Returns anchor pyramid for the given image size.\"\"\"\n",
        "    backbone_shapes = self._compute_backbone_shapes(image_shape)\n",
        "    a = self._generate_pyramid_anchors(RPN_ANCHOR_SCALES, RPN_ANCHOR_RATIOS,\n",
        "                                       backbone_shapes, BACKBONE_STRIDES,\n",
        "                                       RPN_ANCHOR_STRIDE)\n",
        "    a = self._norm_boxes(a, image_shape[:2])\n",
        "    return a\n",
        "\n",
        "\n",
        "  def _apply_box_deltas(boxes, deltas):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      boxes: [batch, k, (y1, x1, y2, x2)]. boxes to update\n",
        "      deltas: [batch, k, (dy, dx, log(dh), log(dw))] refinements to apply\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Convert to bfloat16 for TPU's???\n",
        "    # boxes = boxes.astype(np.float32)\n",
        "    # Convert to y, x, h, w\n",
        "    height = boxes[:, :, 2] - boxes[:, :, 0]\n",
        "    width = boxes[:, :, 3] - boxes[:, :, 1]\n",
        "    center_y = boxes[:, :, 0] + 0.5 * height\n",
        "    center_x = boxes[:, :, 1] + 0.5 * width\n",
        "    # Apply deltas\n",
        "    center_y += deltas[:, :, 0] * height\n",
        "    center_x += deltas[:, :, 1] * width\n",
        "    height *= tf.exp(deltas[:, :, 2])\n",
        "    width *= tf.exp(deltas[:, :, 3])\n",
        "    # Convert back to y1, x1, y2, x2\n",
        "    y1 = center_y - 0.5 * height\n",
        "    x1 = center_x - 0.5 * width\n",
        "    y2 = y1 + height\n",
        "    x2 = x1 + width\n",
        "    result = tf.stack([y1, x1, y2, x2], axis=2, name=\"apply_box_deltas_out\")\n",
        "    return result\n",
        "\n",
        "\n",
        "  def _clip_boxes(boxes, window):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      boxes: [batch, k, (y1, x1, y2, x2)]\n",
        "      window: (y1, x1, y2, x2)\n",
        "    \"\"\"    \n",
        "    # Split\n",
        "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
        "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=2)\n",
        "    # Clip\n",
        "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
        "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
        "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
        "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
        "    clipped = tf.concat([y1, x1, y2, x2], axis=2, name='clipped_boxes')\n",
        "    clipped.set_shape((clipped.shape[0], clipped.shape[1], 4))\n",
        "    return clipped\n",
        "\n",
        "\n",
        "  def _get_proposals(self, rpn_class_probs, rpn_bbox, anchors):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      rpn_class_probs: [batch, num_anchors, (bg prob, fg prob)]\n",
        "      rpn_bbox: [batch, num_anchors, (dy, dx, log(dh), log(dw))]\n",
        "      anchors: [batch, num_anchors, (y1, x1, y2, x2)] anchors in normalized coordinates\n",
        "    \"\"\"\n",
        "\n",
        "    # Box Scores. Use the foreground class confidence. [Batch, num_rois]\n",
        "    scores = rpn_class_probs[:, :, 1]\n",
        "    # Box deltas [batch, num_rois, 4]\n",
        "    deltas = rpn_bbox * np.reshape(RPN_BBOX_STD_DEV, [1, 1, 4])\n",
        "\n",
        "    pre_nms_limit = min(PRE_NMS_LIMIT, anchors.shape[1])\n",
        "\n",
        "    # k = pre_nms_limit, ix = [Batch, k]\n",
        "    ix = tf.math.top_k(scores, pre_nms_limit, sorted=True,\n",
        "                       name='top_anchors').indices\n",
        "\n",
        "    # scores.shape = [Batch, k]\n",
        "    # deltas.shape = pre_nms_anchors.shape = [batch, k, 4]\n",
        "    scores = tf.gather(scores, ix, axis=1, batch_dims=1)\n",
        "    deltas = tf.gather(deltas, ix, axis=1, batch_dims=1)\n",
        "    pre_nms_anchors = tf.gather(anchors, ix, axis=1, batch_dims=1)\n",
        "    \n",
        "    # boxes.shape = [batch, k, 4]\n",
        "    boxes = self._apply_box_deltas(pre_nms_anchors, deltas)\n",
        "    # Clip to image boundaries. Since we're in normalized coordinates,\n",
        "    # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]\n",
        "    window = np.array([0, 0, 1, 1], dtype=np.float32) # TODO: Change to bfloat16?\n",
        "    # boxes = [batch, k, 4]\n",
        "    boxes = self._clip_boxes(boxes, window)\n",
        "\n",
        "    # Extend class dimension for cool tf combined nms function\n",
        "    # boxes.shape = [batch, k, 1, 4], scores.shape = [batch, k, 1]\n",
        "    boxes = tf.expand_dims(boxes, axis=2)\n",
        "    scores = tf.expand_dims(scores, axis=2)\n",
        "    # This function automatically zero pads to POST_NMS_ROIS_TRAINING boxes/batch\n",
        "    proposals, _, _ = tf.image.combined_non_max_suppression(boxes, scores, \n",
        "      POST_NMS_ROIS_TRAINING, POST_NMS_ROIS_TRAINING, RPN_NMS_THRESHOLD)\n",
        "    \n",
        "    return proposals\n",
        "\n",
        "\n",
        "  def _get_detection_targets(proposals, gt_class_ids, gt_boxes, gt_masks):\n",
        "    \"\"\"Generates detection targets for the batch. Subsamples proposals and\n",
        "    generates target class IDs, bounding box deltas, and masks for each.\n",
        "    Inputs:\n",
        "      instances = max(instances/MAX_GT_INSTANCES) (per image)\n",
        "    proposals: [batch, POST_NMS_ROIS_TRAINING, (y1, x1, y2, x2)] in normalized coordinates. Might\n",
        "               be zero padded if there are not enough proposals.\n",
        "    Ragged Tensors:\n",
        "    gt_class_ids: [batch, (instances)] int class IDs\n",
        "    gt_boxes: [batch, (instances), (y1, x1, y2, x2)] in normalized coordinates.\n",
        "    gt_masks: [batch, (instances), height, width] of boolean type.\n",
        "    Returns: Target ROIs and corresponding class IDs, bounding box shifts,\n",
        "    and masks.\n",
        "    rois: [batch, TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)] in normalized coordinates\n",
        "    class_ids: [batch, TRAIN_ROIS_PER_IMAGE]. Integer class IDs. Zero padded.\n",
        "    deltas: [batch, TRAIN_ROIS_PER_IMAGE, (dy, dx, log(dh), log(dw))]\n",
        "    masks: [batch, TRAIN_ROIS_PER_IMAGE, height, width]. Masks cropped to bbox\n",
        "           boundaries and resized to neural network output size.\n",
        "    Note: Returned arrays might be zero padded if not enough target ROIs.\n",
        "    \"\"\"\n",
        "    #Remove Zero Padding\n",
        "    proposals, _ = trim_zeros(proposals, name='trim_proposals')\n",
        "    gt_boxes, non_zeros = trim_zeros(gt_boxes, name='trim_gt_boxes')\n",
        "    gt_class_ids = tf.ragged.boolean_mask(gt_class_ids, non_zeros, \n",
        "                                          name='trim_gt_class_ids')\n",
        "    gt_masks = tf.ragged.boolean_mask(gt_masks, non_zeros,\n",
        "                                      name='trim_gt_masks')\n",
        "    \n",
        "    # Handle COCO crowds\n",
        "    # A crowd box in COCO is a bounding box around several instances. Exclude\n",
        "    # them from training. A crowd box is given a negative class ID.\n",
        "    crowd_mask = (gt_class_ids < 0)\n",
        "    non_crowd_mask = (gt_class_ids > 0)\n",
        "    crowd_boxes = tf.ragged.boolean_mask(gt_boxes, crowd_mask)\n",
        "    gt_class_ids = tf.ragged.boolean_mask(gt_class_ids, non_crowd_mask)\n",
        "    gt_boxes = tf.ragged.boolean_mask(gt_boxes, non_crowd_mask)\n",
        "    gt_masks = tf.ragged.boolean_mask(gt_masks, non_crowd_mask)\n",
        "    \n",
        "    # Compute overlaps matrix [proposals, gt_boxes]\n",
        "    overlaps = overlaps_graph(proposals, gt_boxes)\n",
        "\n",
        "  def call(self, x, train_bn=True, gt_class_ids=None, gt_boxes=None,\n",
        "           gt_masks=None):\n",
        "    \"\"\"\n",
        "    # Arguments:\n",
        "      x: the image tensors, shape = [batch, height, width, channels]\n",
        "      train_bn: Boolean. Train the batch normalization layers?\n",
        "      Only used during training:\n",
        "      gt_class_ids: ground truth classification of instance classes \n",
        "        shape = [batch, (instances/image)] Ragged Tensor (RT)\n",
        "      gt_boxes: ground truth bounding boxes [batch, (instances/image), 4] RT\n",
        "      gt_masks: ground truth segmentation masks \n",
        "        shape = [batch, (instances/image), height, width, ?]\n",
        "    \"\"\"\n",
        "\n",
        "    shape = x.get_shape()\n",
        "    image_shape = shape[1:3]\n",
        "\n",
        "    _, C2, C3, C4, C5 = self._resnet_layer(x, train_bn)\n",
        "    \n",
        "    P5 = self._conv_layer_fpn_c5p5(C5)\n",
        "    P4 = self._upsample_layer_fpn_p5(P5) + self._conv_layer_fpn_c4p4(C4)\n",
        "    P3 = self._upsample_layer_fpn_p4(P4) + self._conv_layer_fpn_c3p3(C3)\n",
        "    P2 = self._upsample_layer_fpn_p3(P3) + self._conv_layer_fpn_c2p2(C2)\n",
        "\n",
        "    #Attach 3x3 conv to all P layers to get the final feature maps\n",
        "    P2 = self._conv_layer_fpn_p2(P2)\n",
        "    P3 = self._conv_layer_fpn_p3(P3)\n",
        "    P4 = self._conv_layer_fpn_p4(P4)\n",
        "    P5 = self._conv_layer_fpn_p5(P5)\n",
        "    # P6 is used for the 5th anchor scale in RPN. Generated by\n",
        "    # subsampling from P5 with stride of 2.\n",
        "    P6 = self._maxpool_layer(P5)\n",
        "\n",
        "    rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
        "    mrcnn_feature_maps = [P2, P3, P4, P5]\n",
        "\n",
        "    if not tuple(image_shape) in self._anchor_cache:\n",
        "      self._anchor_cache[tuple(image_shape)] = self._get_anchors(image_shape)\n",
        "    anchors = self._anchor_cache[tuple(image_shape)]\n",
        "    \n",
        "    # Duplicate across the batch dimension\n",
        "    anchors = np.broadcast_to(anchors, (BATCH_SIZE,) + anchors.shape)\n",
        "\n",
        "    # Loop through pyramid layers\n",
        "    layer_outputs = [] # list of lists\n",
        "    for PX in rpn_feature_maps:\n",
        "      layer_outputs.append(self._RPN(PX))\n",
        "\n",
        "    # Concatenate layer outputs\n",
        "    # Convert from list of lists of level outputs to list of lists\n",
        "    # of outputs across levels.\n",
        "    # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
        "    output_names = [\"rpn_class_logits\", \"rpn_class_probs\", \"rpn_bbox\"]\n",
        "    outputs = list(zip(*layer_outputs))\n",
        "    outputs = [tf.concat(list(o), axis=1) for o, n in zip(outputs, output_names)]\n",
        "\n",
        "    rpn_class_logits, rpn_class_probs, rpn_bbox = outputs\n",
        "\n",
        "    # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]?\n",
        "    proposals = self._get_proposals(rpn_class_probs, rpn_bbox, anchors)\n",
        "\n",
        "    if self.mode == 'training':\n",
        "      \n",
        "      training_rois, target_class_ids, target_bbox, target_mask =\\\n",
        "        self._get_detection_targets(proposals, gt_class_ids, gt_boxes, gt_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}